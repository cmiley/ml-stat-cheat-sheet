[
    {
        "title": "AdaBoost",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Adam Optimizer",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Adversarial Training",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Affine",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Agglomerative Clustering",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Analysis of Variance (ANOVA) & Multivariate Analysis of Variance (MANOVA)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Attention",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Autocorrelation",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Autoencoders",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Automatic Machine Learning (AutoML)",
        "description": "Automatic Machine Learning is the process of automating the model training and prediction process. The purpose is to remove variables such as hyperparameter tuning and model selection. In some cases, it is possible to generate a model without labeled data, though this is still some form of clustering or similarity measure. AutoGluon is a useful open source package for AutoML. Additionally, Google, Amazon, Microsoft, etc. offer AutoML services that can operate over tabular data, images, free text, or a combination.",
        "tags": ["automl", "autogluon", "machine learning"],
        "equation": null,
        "resources": ["https://www.automl.org/automl/"]
    },
    {
        "title": "Average Linkage",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Back-Propagation",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Bagging",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Bayes Filter",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Bayes Rule",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Bayes Search",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Bayesian Data Analysis (BDA)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Bernoulli Distribution",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Bi-plots",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Bilingual Evaluation Understudy (BLEU)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Binomial Distribution",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Boltzmann Machines and Restricted Boltzmann Machines (RBM)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Boosting",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Canberra Metric",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Cardinality",
        "description": "Cardinality is the number of elements in a set. It is particularly useful in machine learning when describing the possible feature values that a set of samples can contain. For example, consider the context of a color feature: all hex color codes possible are greater than 16 million, but a representative sample may contain significantly less than that, especially if the feature can be considered categorical.",
        "tags": ["mathematics", "machine learning"],
        "equation": null,
        "resources": []
    },
    {
        "title": "Categorical Encoding",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Cauchy-Schwarz Inequality",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Central Limit Theorem",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Chain Rule for Probability",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Cholesky Factorization",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": ["https://www.maths.manchester.ac.uk/~higham/papers/high09c.pdf"]
    },
    {
        "title": "Cholesky Norm",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Classification vs. Regression",
        "description": "Classification is generally attempting to predict a discrete set of values as opposed to regression which attempts to predict a real value (a continuous set of values). Making this distinction on your labels is useful in model selection and evaluation of techniques for data transformation.",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Complete Linkage",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Complimentary Slackness Theorem",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Conditional Probability",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Confidence Intervals",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Contrastive Divergence",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Convex Optimization",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Convolutional Filter",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Convolutional Neural Network (CNN)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Correspondence Analysis",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Cross Entropy",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Cross Validation",
        "description": "Cross validation is the process of validating a model on a dataset without a specified validation set. The process usually involves splitting the training dataset into \"folds\" where a portion of the training data remains as training data and another portion is used as a validation set. This allows for a model to use the entire training dataset for training. Some complexity is introduced in guaranteeing that predictions on the validation dataset are in fact out of sample. A property of cross validation is that every sample in the training dataset can get an out of sample prediction, which is useful for further analysis. Cross validation is generally more common on smaller datasets, where the luxury of a representative validation dataset cannot be achieved. Multiple models are produced via cross validation, so a complete model for production or test set prediction then becomes an ensemble of the models trained during cross validation, or a final model using the training process that produced the best model can be trained on the entire training dataset without validation.",
        "tags": [],
        "equation": null,
        "resources": ["https://scikit-learn.org/stable/modules/cross_validation.html", "https://en.wikipedia.org/wiki/Cross-validation_(statistics)"]
    },
    {
        "title": "Czekanowski Coefficient",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": ["https://ag.arizona.edu/classes/rnr555/lecnotes/10.html"]
    },
    {
        "title": "Data Shift",
        "description": "Data shift refers to the shift in the distribution of data over time, especially in reference to a prediction set after a model has been trained. It is especially important to monitor data shift when using models in production to verify that the training data is maintaining representation of the test population. Quantifying data shift is generally good evidence that can be used to determine how often a model should be re-trained and when data becomes stale for model training.",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Decision Tree",
        "description": "A decision tree is a simple deterministic model that can operate as a classifier or regressor depending on the implementation.",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Deep Belief Network",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Deep Feedforward Networks",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Deep Generative Modeling",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Definition of Convex",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Difference Between Learning and Optimization",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Dirichlet Process Modeling",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Dropout",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Early Stopping",
        "description": "Early stopping is a process used during model training, usually for gradient-based models, that stops model training prior to a threshold completion to prevent overfitting. A common approach is to use a stopping parameter based on a threshold for validation loss, for example only training 3 epochs after validation loss stops decreasing by a certain magnitude. Another approach is as a hyperparameter for a simple model, such as setting a max depth for the decision trees used in a random forest.",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Echo State Networks",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Edit Distance",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Efficient Convolutions",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Empirical Risk Minimization",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Encoder-Decoder Sequence to Sequence",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Ensemble Methods",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Erlang Distribution",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Euclidean Distance",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Expectation Maximization",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Expectation Propagation",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Exploding Gradients",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Exponential Distribution",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "F-Measure",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Factor Analysis",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Factorial Experiments",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Feature Pyramid Network",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Few-Shot Learning",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Finite Mixture Modeling",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Fisher's Method",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Gamma Distribution",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Gated Recurrent Unit (GRU)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Gaussian Process Regression",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Generative Adversarial Network (GAN)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Geometric Distribution",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Gibbs Sampler",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Graph Convolution",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Graph Neural Networks (GNN)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Grid Search",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Hamiltonian Monte Carlo",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Hyper-geometric Distribution",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Hyperparameter Tuning",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Hyperparameters",
        "description": "Hyperparameters are meta parameters related to model training - often selected by the modeler via intuition. Examples include learning rate, selection of the model itself, dropout, etc. Automatic tuning of hyperparameters is also common in AutoML or during the process of iterative model training, usually by constraining the hyperparameters via a function or range.",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Hypothesis Testing",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Importance Sampling",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Impurity Importance",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Independent Component Analysis",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Integer Programming",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Interpolation",
        "description": "Interpolation is the process of interpreting missing data by assuming a function that matches data points. A common approach is to use linear interpolation for time series data points. This operates on a core assumption that the data used is continuous in nature.",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Jensen-Shannon (JS) Divergence",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Kalman Filter",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Kernel",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Kullback-Leibler (KL) Divergence",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Laplacian/Laplace Transform",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Lasso Regression",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Latent Gaussian Process",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Leaky Units",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Least Squares Estimation",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Lemke's Algorithm",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Levenshtein Distance",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Linear Complimentarity Programming",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Linear Discriminant Analysis",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Linear Fractional Programming",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Linear Regression",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Localization",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Log-Likelihood Gradient",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Log-normal Distribution",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Long-Short Term Memory (LSTM) and Gated Units",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Lower-Upper Decomposition",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Manifold",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Manifold Hypothesis",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Marginal Probability",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Markov Chain",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Maximum Likelihood Estimation (MLE)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Mean Absolute Error (MAE)",
        "description": "Mean absolute error is an error metric that is calculated by taking the absolute difference between prediction and label, then computing the mean of the errors.",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Mean Squared Error (MSE)",
        "description": "Mean squared error is an error metric that is calculated by taking the squared difference between prediction and label, then computing the mean of the errors. This penalizes larger magnitude errors much more harshly than MAE.",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Metric Learning",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Metric for Evaluation of Translation with Explicit Ordering (METEOR)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Metropolis and Metropolis-Hastings Algorithm",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Minkowski Metric",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Model Pruning",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Model Shift",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Momentum",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Moore-Penrose Pseudoinverse",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Multi-Dimensional Scaling",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Multimodal",
        "description": "Multimodal is a descriptor that means the model or dataset has multiple \"modes\" (sometimes also referred to as domains). A simple example is a dataset which contains samples with both images and free text. A multimodal model is usually a fusion of state of the art techniques on each mode, for example a CNN-based approach on image data and a transformer-based approach on free text data, which are then fused (concatenated output in the simplest case) and a further model is used to produce a complete prediction per sample.",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Multinomial Distribution",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Multiple Imputation",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Multitask Learning",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Multivariate Multiple Regression",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Negative Binomial Distribution",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Neural Architecture Search (NAS)",
        "description": "A neural architecture search is the process of determining a potentially ideal architecture for a neural network. The process usually involves searching over a space constructed of possible neural building blocks (such as different types of neural network layers) and a stopping parameter. This process is generally computationally expensive since the search space is infinite and each iteration requires training a model. To decrease computation time, candidate models are trained very quickly and used for future iterations in a Bayesian or Evolutionary computing fashion based on their performance.",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Newton's Method",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Noise Robustness",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Normal Distribution",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Normalization",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Numerical Integration",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Operations Research",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Operations that Preserve Convexity",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Overfitting",
        "description": "Overfitting is a term used to describe when a model no longer generalizes to out of sample predictions because it has too heavily weighted performance on the training data. Overfitting reduction is a core principle in machine learning, most simply achieved by using a holdout set of of samples for validation, which the model is not directly trained on but is used to evaluate model performance during training.",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Overflow",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Parameter Norm Penalty",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Partition Function",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Perceptron",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Permutation Importance",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Perturbation and Sensitivity Analysis",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Poisson Distribution",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Pooling",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Popular CNNs",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Precision",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Precision-Recall (PR) Curve",
        "description": "A precision-recall curve is a graph used for evaluation of thresholds for discretizing predictions. It is a piece-wise function that demonstrates the trade-off between precision and recall on a set of data by varying the threshold at which a continuous prediction is bucketed into a binary classification. This is a useful measure for attempting to achieve a target precision or recall based on requirements for the model performance in production.",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Principal Component Analysis (PCA)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Procrustes Analysis",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Pseudolikelihood",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Quadratic Programming",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Quaternion",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Random Forest (Mean Trees)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Random Forest (Quantile Trees)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Recall",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Receiver Operating Characteristic (ROC) Curve",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Rectified Linear Unit (ReLU)",
        "description": "A rectified linear unit is a common non-linear activation function used between feed forward network layers. Its most important property is that it only produces non-negative values and otherwise does not change the input.",
        "tags": [],
        "equation": "ReLU(x) = max(0, x)",
        "resources": []
    },
    {
        "title": "Recurrent Neural Network (RNN)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Regularization",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Representation Learning",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Risk vs. Empirical Risk",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Robust Inference",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Root Mean Squared Error (RMSE)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Sampling",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Segmentation",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Shannon Entropy",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Shannon Information",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Shapley Additive Explanations (SHAP)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Siamese Model",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Sigmoid",
        "description": "Sigmoid is a common non-linear activation function that constrains values to be valued between 0 and 1 inclusively. It is most common in binary classification to constrain continuous values to an easily thresholded value to produce a legitimate classification prediction. The output of a sigmoid is often referred to as the probability that a prediction belongs in a classification bucket, but it is not a true probability - it is instead a pseudoprobability because it is produced by the model output rather than via traditional probabilistic means.",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Singular Value Decomposition (SVD)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Sparse Coding",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Spectral Decomposition",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Splines and Weighted Sums",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Stochastic Gradient Descent (SGD)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Student-Teacher Distillation",
        "description": "Student-teacher distillation is the process of using a trained model as a \"teacher\" to constrain the training of a new model, the \"student\" in this case. The purpose is to have the student model produce similar (or even identical) outputs as the teacher model without maintaining the architecture (or even the same model selection) as the teacher. It is commonly used to transfer model performance to a new model type or reduce the physical disk size of a deep model. A useful example is training a complete model using a large dataset and constraining the student to be less than the size of an edge device that will use the model in production, such as a model that is used for offline predictions on a phone.",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Support Vector Machine (SVM)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Tangent Distance, Prop, and Manifold Tangent",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Tensor",
        "description": "A tensor is a generalized form of a multi-dimensional matrix. It is useful in machine learning due to the high-dimensional data and spaces often used for data visualization and model representation.",
        "tags": ["mathematics"],
        "equation": null,
        "resources": ["https://mathworld.wolfram.com/Tensor.html"]
    },
    {
        "title": "Trace Operator",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Transfer Learning",
        "description": "Transfer learning is the process of using a previously trained model on a different set of data and using it as an initialization for training on a new set of data. This is extremely common for image datasets, where the original dataset was vastly large and it can be assumed that the core structure of the model has a good understanding of ALL images that is easily generalizable to a specific subset of images. A common approach is to fine-tune a model by freezing layers or using much smaller learning rate during training on the target dataset.",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Transformer",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Triplet Loss",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "U-Net",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Underfitting",
        "description": "Underfitting is a term used for describing when a model is incapable of learning a representation of the sample dataset. A simple example is predicting the mean value of a label set for all predictions. While performance may be reasonable, nothing is learned and there is no possibility for generalization. In some cases, this can be useful for selecting particularly simple models that fit for certain datasets.",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Underflow",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Uniform Distribution",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Universal Manifold Approximation and Projection (UMAP)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Variational Inference",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Ward's Hierarchical Clustering",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "Weibull Distribution",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "You Only Look Once (YOLO)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "k-Means Clustering",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "k-Nearest Neighbors (KNN)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    },
    {
        "title": "t-Distributed Stochastic Neighbor Embedding (t-SNE)",
        "description": "",
        "tags": [],
        "equation": null,
        "resources": []
    }
]